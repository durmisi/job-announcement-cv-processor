import os
import requests
from . import LLMClient

class OllamaClient(LLMClient):
    def __init__(self):
        self.base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")

    def generate_response(self, prompt: str) -> str:
        response = requests.post(
            f"{self.base_url}/api/generate",
            json={"model": "llama3.2", "prompt": prompt, "stream": False}
        )
        response.raise_for_status()
        return response.json()["response"]

# Generated by Copilot