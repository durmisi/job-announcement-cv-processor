import os
from .openai_client import OpenAIClient
from .azure_client import AzureOpenAIClient
from .ollama_client import OllamaClient
from .github_client import GitHubModelsClient

def get_llm_client(provider: str = None):
    if provider is None:
        provider = os.getenv("LLM_PROVIDER", "ollama").lower()
    if provider == "openai":
        return OpenAIClient()
    elif provider == "azure":
        return AzureOpenAIClient()
    elif provider == "ollama":
        return OllamaClient()
    elif provider == "github":
        return GitHubModelsClient()
    else:
        raise ValueError(f"Unsupported LLM provider: {provider}")

# Generated by Copilot