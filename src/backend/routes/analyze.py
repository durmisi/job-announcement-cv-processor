from fastapi import APIRouter, HTTPException, UploadFile, File, Form
import json
import logging
from llm.factory import get_llm_client

router = APIRouter()

logger = logging.getLogger(__name__)

@router.post("/analyze")
async def analyze_cv(cv_file: UploadFile = File(...), job_content: str = Form(...)):
    if not cv_file:
        raise HTTPException(status_code=400, detail="CV file is required")
    if not job_content.strip():
        raise HTTPException(status_code=400, detail="Job content is required")
    
    # Read CV content
    cv_content = (await cv_file.read()).decode("utf-8")
    
    logger.info(f"Received CV file: {cv_file.filename}, size: {len(cv_content)} bytes")
    logger.info(f"Job content length: {len(job_content)}")
    
    # Create prompt for LLM
    prompt = f"""
Analyze the following CV content against the job description and provide a detailed analysis in JSON format.

CV Content:
{cv_content}

Job Description:
{job_content}

Please respond with a JSON object containing:
- score: integer (0-100)
- summary: string
- strengths: array of strings
- gaps: array of strings
- skills: array of objects with "name" and "status" (strong, partial, missing)
- experience: array of objects with "category" and "match" (0-100)
- recommendations: array of strings

Ensure the response is valid JSON.
"""
    
    logger.info("Creating prompt for LLM")
    
    try:
        client = get_llm_client()
        logger.info("Calling LLM for analysis")
        response = client.generate_response(prompt)
        logger.info("LLM response received, parsing JSON")
        analysis = json.loads(response)
        logger.info(f"Analysis completed with score: {analysis.get('score', 'N/A')}")
        return analysis
    except Exception as e:
        logger.error(f"LLM analysis failed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"LLM analysis failed: {str(e)}")

# Generated by Copilot